{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/bened/Documents/After Masters Applications/Careers/Citadel Datathon')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "#nrows = 70000\n",
    "data = pd.read_csv('test_packages.csv')\n",
    "headlines = data['headline']\n",
    "test_ids = data['test_id']\n",
    "impression_click_ratio = data['clicks'].divide(data['impressions'])\n",
    "first_place = data['first_place']\n",
    "highest = data['high_ctr_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What They Learned From The Scientist Was Terrifying. But That's What They Needed To Hear.\", \"A Science Guy Helps 3 Dudes From America Understand What They're Really Up Against\"]\n"
     ]
    }
   ],
   "source": [
    "print((headlines.tolist()[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Building Model -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/pytorch_model.bin from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\2e4719cf8d097772eb75070b88cbc56f1d3b1392fffc5f75032a389ef21d1847.16366ca1277caccb15200478349503b3336a1420ac26d44fc16763354f5a2cae\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/vocab.txt from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\973dbacfdf4c488622f01d1a226089e9e3dba130a0c3c11c2e36d49466fa40a8.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/bpe.codes from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\0e474c44ff353f3b378fb140e7e6d4431df4ec6142e8b38d584c0dbc5afc3521.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/added_tokens.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\fe46927817477a58ec2aa92ef52f8ee6fc9e824d054f4aa6a3c129724dc9c9b7.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/special_tokens_map.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\9413ac0bed76140860deffa0c5a29ee4da7d49a3810da1b4b51b27f790bc9255.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/tokenizer_config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\61374b71c02fdfd2929a3cdce24c242049e036624e15e18461a3a70cfc35e939.c260b44e952f7f2a825aac395f2ebbed4ac9553800d1e320af246e81a548f37c\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\cb09766f7ba60b5f7a1bb640617b24f1499c4a6f3ab160c4a0ac171e3a377c68.008dca06003188334001a96363da79ced4944abc68d94a2f1e0db786dc5aa08b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Computing Sentiments -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 72135/72135 [1:23:02<00:00, 14.48it/s]\n"
     ]
    }
   ],
   "source": [
    "## import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate model\n",
    "print('----- Building Model -----')\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "#nlp = AutoModelForSequenceClassification.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "nlp = pipeline('sentiment-analysis', model='finiteautomata/bertweet-base-sentiment-analysis') # there is also emotion analysis\n",
    "\n",
    "print('----- Computing Sentiments -----')\n",
    "# Compute sentiments\n",
    "sentiments = []\n",
    "for headline in tqdm(headlines):\n",
    "    sentiments.append(nlp(headline, return_all_scores=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiments to data\n",
    "_sentiments = [i[0] for i in sentiments]\n",
    "sent_list = [{'neg': neg['score'], 'neu': neu['score'], 'pos': pos['score']} for neg, neu, pos in _sentiments[:]]\n",
    "sent_df = pd.DataFrame(sent_list)\n",
    "df = pd.concat([data, sent_df], axis=1)\n",
    "df.to_csv('test_packages_with_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Picked packages -----\n",
      "Negative Ratio: 0.43975941753719533\n",
      "Neutral Ratio: 0.3974042418486863\n",
      "Positive Ratio: 0.1628363406141184\n",
      "\n",
      "----- Non-Picked packages -----\n",
      "Negative Ratio: 0.395580404685836\n",
      "Neutral Ratio: 0.4289847355342563\n",
      "Positive Ratio: 0.1754348597799077\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('test_packages_with_sentiment.csv')\n",
    "sentiment = {'neg':df['neg'],'neu':df['neu'],'pos':df['pos']}\n",
    "picked = df['high_ctr_test']\n",
    "\n",
    "picked_sentiment = []\n",
    "not_picked_sentiment = []\n",
    "picked_scores = {'neg':[],'neu':[],'pos':[]}\n",
    "not_picked_scores = {'neg':[],'neu':[],'pos':[]}\n",
    "for i in range(len(picked)):\n",
    "    sent = ['neg','neu','pos'][np.argmax([sentiment['neg'][i],sentiment['neu'][i],sentiment['pos'][i]])]\n",
    "    if picked[i] == True:\n",
    "        picked_sentiment.append(sent)\n",
    "        picked_scores['neg'].append(sentiment['neg'][i])\n",
    "        picked_scores['neu'].append(sentiment['neu'][i])\n",
    "        picked_scores['pos'].append(sentiment['pos'][i])\n",
    "    elif picked[i] == False:\n",
    "        not_picked_sentiment.append(sent)\n",
    "        not_picked_scores['neg'].append(sentiment['neg'][i])\n",
    "        not_picked_scores['neu'].append(sentiment['neu'][i])\n",
    "        not_picked_scores['pos'].append(sentiment['pos'][i])\n",
    "\n",
    "print('----- Picked packages -----')\n",
    "print('Negative Ratio:', picked_sentiment.count('neg') / len(picked_sentiment))\n",
    "print('Neutral Ratio:', picked_sentiment.count('neu') / len(picked_sentiment))\n",
    "print('Positive Ratio:', picked_sentiment.count('pos') / len(picked_sentiment))\n",
    "\n",
    "print()\n",
    "print('----- Non-Picked packages -----')\n",
    "print('Negative Ratio:', not_picked_sentiment.count('neg') / len(not_picked_sentiment))\n",
    "print('Neutral Ratio:', not_picked_sentiment.count('neu') / len(not_picked_sentiment))\n",
    "print('Positive Ratio:', not_picked_sentiment.count('pos') / len(not_picked_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f077ac26a7a74a5aa668136acccd4b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9e3a66deae444b9c27e4973e3e5565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc72ba06e2064134899169611a234e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e190dac4a8de46c19fd688fac1569a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdb0b503b814bccbca098861c5c66e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627da4f80bba4710a116d65312fa4188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea254bacbb349fb8c95da8ff959740c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec73b08069c4d9bb0c07e66401aeb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc2c137e6de4ab7a458b78861c68db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d86af76869f4788a8de0dbbe42c60d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6907a35f45d545e2adb7cc53df52ff0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7115fe4871004445802fb7deacd1f73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2741d594c70140b8a416a1a12119b945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d206df71de14c6a9be50d257e49dc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Didn't find file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\added_tokens.json. We won't load it.\n",
      "loading file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\vocab.txt\n",
      "loading file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\tokenizer.json\n",
      "loading file None\n",
      "loading file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\special_tokens_map.json\n",
      "loading file C:\\Users\\bened/.cache\\torch\\sentence_transformers\\sentence-transformers_all-MiniLM-L6-v2\\tokenizer_config.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-1ad1b328fe06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# storing embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Documents/After Masters Applications/Careers/Citadel Datathon/Untitled1.ipynb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m# loading embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# hl_embeddings = pickle.load(open('/path/to/file', \"rb\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'obj' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embedding function \n",
    "\n",
    "def bert(sentences: List[str]) -> List[np.array]:\n",
    "    \"\"\"Compute bert embeddings for nodes of a knowledge graph\n",
    "\n",
    "    Args:\n",
    "        sentences: text to use for each node\n",
    "\n",
    "    Returns:\n",
    "        A map nodes and their embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    # Compute embedding for both lists\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True, batch_size=32)\n",
    "\n",
    "    return [embed.cpu().numpy() for embed in embeddings]\n",
    "\n",
    "# compute embeddings\n",
    "headlines_inp = list(headlines)\n",
    "hl_embeddings = bert(headlines_inp) # list of embeddings\n",
    "hl_embeddings = np.array(hl_embeddings) # matrix of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing embeddings\n",
    "pickle.dump(hl_embeddings, open('C:/Users/bened/Documents/After Masters Applications/Careers/Citadel Datathon/hl_embeddings.emb', \"wb\"))\n",
    "# loading embeddings\n",
    "# hl_embeddings = pickle.load(open('/path/to/file', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert percentages (between 0 and 1) to arcisn values (between -infinity and infinity) in order to use normal distribution\n",
    "def arcsin_percentage(inp_dict):\n",
    "    out_dict = {}\n",
    "    for key in inp_dict.keys():\n",
    "        out_dict[key] = np.arcsin(2*(np.array(inp_dict[key])) - 1)\n",
    "    return out_dict\n",
    "\n",
    "xs_picked = arcsin_percentage(picked_scores)\n",
    "xs_not_picked = arcsin_percentage(not_picked_scores)\n",
    "\n",
    "mean_picked = {}\n",
    "stddev_picked = {}\n",
    "\n",
    "mean_picked['neg'] = np.mean(xs_picked['neg'])\n",
    "mean_picked['neu'] = np.mean(xs_picked['neu'])\n",
    "mean_picked['pos'] = np.mean(xs_picked['pos'])\n",
    "stddev_picked['neg'] = np.std(xs_picked['neg'])\n",
    "stddev_picked['neu'] = np.std(xs_picked['neu'])\n",
    "stddev_picked['pos'] = np.std(xs_picked['pos'])\n",
    "\n",
    "mean_not_picked = {}\n",
    "stddev_not_picked = {}\n",
    "\n",
    "mean_not_picked['neg'] = np.mean(xs_not_picked['neg'])\n",
    "mean_not_picked['neu'] = np.mean(xs_not_picked['neu'])\n",
    "mean_not_picked['pos'] = np.mean(xs_not_picked['pos'])\n",
    "stddev_not_picked['neg'] = np.std(xs_not_picked['neg'])\n",
    "stddev_not_picked['neu'] = np.std(xs_not_picked['neu'])\n",
    "stddev_not_picked['pos'] = np.std(xs_not_picked['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': -0.2602445292832059, 'neu': -0.2533461049888791, 'pos': -0.906163122004667}\n",
      "{'neg': 1.0611369871718177, 'neu': 0.8523177832936591, 'pos': 0.8477971819432795}\n",
      "\n",
      "{'neg': -0.3599007669972695, 'neu': -0.19822545177525383, 'pos': -0.8643338821470694}\n",
      "{'neg': 1.0519553958964454, 'neu': 0.85860721169747, 'pos': 0.8663815225298086}\n"
     ]
    }
   ],
   "source": [
    "print(mean_picked)\n",
    "print(stddev_picked)\n",
    "print()\n",
    "print(mean_not_picked)\n",
    "print(stddev_not_picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=10.50181425064582, pvalue=8.844763721251118e-26)\n",
      "Ttest_indResult(statistic=-7.141742169152008, pvalue=9.302218166234823e-13)\n",
      "Ttest_indResult(statistic=-5.387501261434515, pvalue=7.166704679186783e-08)\n"
     ]
    }
   ],
   "source": [
    "import scipy as scipy\n",
    "\n",
    "result_neg = scipy.stats.ttest_ind(xs_picked['neg'], xs_not_picked['neg'])\n",
    "print(result_neg)\n",
    "\n",
    "result_neu = scipy.stats.ttest_ind(xs_picked['neu'], xs_not_picked['neu'])\n",
    "print(result_neu)\n",
    "\n",
    "result_pos = scipy.stats.ttest_ind(xs_picked['pos'], xs_not_picked['pos'])\n",
    "print(result_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/pytorch_model.bin from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\61c5894a0aca5ed63159e2ec6a5501db48124c1e6de287b82bc634334f031203.9c3c4c16d0dd174434d42471b9d4670734d982be506a06fc3111c12bee4380c7\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/vocab.txt from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\b7837213513a9f3852dcb04048f70c13cbd0590be030e534734ffd42cbdcf45a.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/bpe.codes from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\76e357e2554ebe053d1c4c613506bc2cc19d66ae27fec8218261a7f73c6456b9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/added_tokens.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\c4b7522f44ed8adb95e62288c6458da591654f7466e3ce2f9c730bb4087411d2.c1e7052e39d2135302ec27455f6db22e1520e6539942ff60a849c7f83f8ec6dc\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/special_tokens_map.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\01581144d9bf96cb9c7d8a77ee93c8b1f1095af5c1204b1b038a8cb0e3247aa8.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer_config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\1740697312c59fe96586f476c7765cd6f08516a6102ea96f22ffee64f7553234.c260b44e952f7f2a825aac395f2ebbed4ac9553800d1e320af246e81a548f37c\n",
      "loading file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/finiteautomata/bertweet-base-emotion-analysis/resolve/main/config.json from cache at C:\\Users\\bened/.cache\\huggingface\\transformers\\c246eed05359b1a49c45955b0265b488e35b0cbd2628e3ead7dd54c8815162ee.a2dff24b4e0a884c6d58a09968c5b68e7391e749eb698ad92541818d420fd01b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 70000/70000 [1:23:21<00:00, 14.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pysentimiento import EmotionAnalyzer\n",
    "from tqdm import trange,tqdm\n",
    "\n",
    "# Instantiate model\n",
    "#print('----- Building Model -----')\n",
    "#analyzer = EmotionAnalyzer(lang=\"en\")\n",
    "\n",
    "emotions = []\n",
    "model = pipeline('text-classification', model='finiteautomata/bertweet-base-emotion-analysis') \n",
    "for headline in tqdm(headlines):\n",
    "    emotions.append(model(headline, return_all_scores=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print('----- Computing Emotions -----')\n",
    "emotions = []\n",
    "for i in trange(len(headlines[:5])):\n",
    "    emotions.append(analyzer.predict(headlines.tolist()[i]))'''\n",
    "\n",
    "emot_df = pd.DataFrame(emot_list)\n",
    "_emotions = [i[0] for i in emotions]\n",
    "emot_list = [{'others': others['score'], 'joy': joy['score'], 'sadness': sadness['score'], 'anger': anger['score'], 'surprise': surprise['score'], 'disgust': disgust['score'], 'fear': fear['score']} for others, joy, sadness, anger, surprise, disgust, fear in _emotions[:]]\n",
    "emot_df = pd.DataFrame(emot_list)\n",
    "df = pd.concat([data, emot_df], axis=1)\n",
    "df.to_csv('test_packages_with_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('test_packages_with_emotions.csv')\n",
    "emotion = {'others': df2['others'], 'joy':df2['joy'], 'sadness': df2['sadness'], 'anger': df2['anger'], 'surprise': df2['surprise'], 'disgust': df2['disgust'], 'fear': df2['fear']}\n",
    "picked = df['high_ctr_test']\n",
    "\n",
    "picked_emotion = []\n",
    "not_picked_emotion = []\n",
    "for i in range(len(picked)):\n",
    "    emot = ['others','joy','sadness', 'anger', 'surprise', 'disgust', 'fear'][np.argmax([emotion['others'][i],emotion['joy'][i],emotion['sadness'][i],emotion['anger'][i],emotion['surprise'][i],emotion['disgust'][i],emotion['fear'][i]])]\n",
    "    if picked[i] == True:\n",
    "        picked_emotion.append(emot)\n",
    "    elif picked[i] == False:\n",
    "        not_picked_emotion.append(emot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def div_d(my_dict):\n",
    "    sum_p = sum(my_dict.values())\n",
    "    for i in my_dict:\n",
    "        my_dict[i] = float(my_dict[i]/sum_p)\n",
    "    return my_dict \n",
    "\n",
    "picked_densities = {}\n",
    "not_picked_densities = {}\n",
    "classes = ['others','joy','sadness', 'anger', 'surprise', 'disgust', 'fear']\n",
    "classes_ = ['Joy','Sadness', 'Anger', 'Surprise', 'Disgust', 'Fear'] \n",
    "\n",
    "for clas in classes:\n",
    "    picked_densities[clas] = picked_emotion.count(clas)\n",
    "    not_picked_densities[clas] = not_picked_emotion.count(clas)\n",
    "    \n",
    "picked_densities = div_d(picked_densities)\n",
    "not_picked_densities = div_d(not_picked_densities)\n",
    "order = np.arange(len(classes_))\n",
    "\n",
    "plt.bar(order, list(picked_densities.values())[1:], width=0.3, color='cyan', align='center', label = 'Picked', alpha=0.4)\n",
    "plt.bar(np.array(order)+0.3, list(not_picked_densities.values())[1:], width=0.3, color='red', align='center', label = 'Not Picked', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.xticks(order,classes_)\n",
    "plt.ylabel('Percentage Occurrence')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([0.6764361078546307, 0.05282011202292562, 0.03328122964699753, 0.0021492770613520907, 0.08160739872345968, 0.14120098997004038, 0.012504884720593983])\n",
      "dict_values([0.706950188485891, 0.058009735387768546, 0.031823006258463564, 0.0032024301870219227, 0.05650916810013542, 0.13296490136515024, 0.010540570215569301])\n"
     ]
    }
   ],
   "source": [
    "print(picked_densities.values())\n",
    "print(not_picked_densities.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
