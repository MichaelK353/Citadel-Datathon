{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "245baec1-5fdb-4cac-bfe4-3413be67e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8365a5-cfe4-40af-8585-2292db320cbb",
   "metadata": {},
   "source": [
    "# Compute Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f1a4658-6f76-411f-b309-5f485f610e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src import embeddings\n",
    "\n",
    "df = pd.read_csv('../output/armando_all_features_global_prediction.csv')\n",
    "headlines = list(df['headline'])\n",
    "# hl_embeddings = embeddings.bert(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9f48f34-d59b-44a0-9922-00cc740b70c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.252049\n",
       "1        0.520419\n",
       "2        0.513248\n",
       "3        0.134193\n",
       "4       -0.041614\n",
       "           ...   \n",
       "59995   -0.272440\n",
       "59996   -0.546174\n",
       "59997   -0.804875\n",
       "59998    1.539790\n",
       "59999   -0.148176\n",
       "Name: ctr, Length: 60000, dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize df['ctr']\n",
    "mu = df['ctr'].mean()\n",
    "sigma = df['ctr'].std()\n",
    "\n",
    "df['ctr']=(df['ctr']-mu)/sigma\n",
    "df['ctr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1599ed4a-6751-4c94-8640-6bb3f9e3d5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013374298447697833, 0.011262850831548153)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "100e4c0e-94e2-4cdb-9181-877b45c2902c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 384)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "hle_fname = '../output/hl_embeddings_armando_all_features_global_prediction.pkl'\n",
    "hl_embeddings = pickle.load(open(hle_fname, \"rb\"))\n",
    "hl_embeddings = np.array(hl_embeddings)\n",
    "hl_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f04c776c-db84-4099-a378-1f36d761bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 384)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from src import utils\n",
    "# utils.write_pkl(hl_embeddings, '../output/hl_embeddings_armando_all_features_global_prediction.pkl')\n",
    "# hl_embeddings = np.array(hl_embeddings)\n",
    "# hl_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e0bb2cc-2cd0-447b-8e18-dabf257481f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "# df['test_week_str'] = df['test_week'].apply(lambda x: f'{x//100}-W{x%100}')\n",
    "# df['test_week_dt'] = df['test_week_str'].apply(lambda x: datetime.datetime.strptime(x + '-1', \"%Y-W%W-%w\"))\n",
    "# df[['test_week_str', 'test_week_dt']]\n",
    "df['test_week'] = df['test_week'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7d35db3-d461-4e01-b573-a44a0918d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def week_of_month(dt):\n",
    "    \"\"\" Returns the week of the month for the specified date.\n",
    "    \"\"\"\n",
    "\n",
    "    first_day = dt.replace(day=1)\n",
    "\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "\n",
    "    return int(ceil(adjusted_dom/7.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0230d210-7bd7-4354-8814-3b8b8e57d527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list with seconds since earliest event\n",
    "date_features = list(df['test_week']) # list(df['test_week_dt'])\n",
    "# date_features = [(i - min(date_features)).total_seconds() for i in date_features]\n",
    "# # Normalize data so it lies between 0 and 1\n",
    "# date_features = [i/max(date_features) for i in date_features]\n",
    "date_features = [[test_week.month, week_of_month(test_week)] for test_week in date_features] # add week of month and  month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f3aad087-f0eb-4c1a-b910-ed693adbecc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      6  2\n",
       "1      6  4\n",
       "2      6  4\n",
       "3      6  4\n",
       "4      8  2\n",
       "...   .. ..\n",
       "59995  4  5\n",
       "59996  6  6\n",
       "59997  9  4\n",
       "59998  4  5\n",
       "59999  9  1\n",
       "\n",
       "[60000 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_df = pd.DataFrame(date_features)\n",
    "dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6727b4ea-c05a-4a41-ab55-c63474713675",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_df = pd.get_dummies(dates_df[0])\n",
    "weeks_df = pd.get_dummies(dates_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbba9930-ae15-4e61-85a6-1dae881d0e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9     7398\n",
       " 10    5985\n",
       " 8     5560\n",
       " 6     5308\n",
       " 4     5160\n",
       " 3     5130\n",
       " 7     5064\n",
       " 11    4975\n",
       " 12    4679\n",
       " 2     4259\n",
       " 1     3693\n",
       " 5     2789\n",
       " Name: 0, dtype: int64,\n",
       " 3    14473\n",
       " 4    14114\n",
       " 5    13692\n",
       " 2    13385\n",
       " 1     2448\n",
       " 6     1888\n",
       " Name: 1, dtype: int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_df[0].value_counts(), dates_df[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9cacb4c-641c-45d5-b226-bd7d48c7a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_embeddings, week_embeddings = months_df.to_numpy(), weeks_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f73aadb-f112-4b80-b928-f3adc0bba866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 384), (60000, 12), (60000, 6))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_embeddings.shape, months_embeddings.shape, week_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2d76c719-e8e1-4dcc-a480-fe670f110624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_keys = ['LOC', 'PERSON', 'WORK_OF_ART','LANGUAGE','PRODUCT','QUANTITY','DATE','CARDINAL','GPE','TIME','ORG','LAW','MONEY','FAC','EVENT','PERCENT','NORP','ORDINAL']\n",
    "ner_cols = [f'{k}_start' for k in ner_keys] # + [f'{k}_end' for k in ner_keys]\n",
    "other_features = ['hl_len', 'neg', 'neu', 'pos'] + ner_cols\n",
    "df[ner_cols] = df[ner_cols].astype('bool').astype('int')\n",
    "\n",
    "other_embeddings = df[other_features].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "85db8a10-cca8-4227-9a7d-5bba1fe66ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "98d581ae-ff04-4dcd-a140-61140fd2ef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 36)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a2aeb989-e2d7-4b7c-b866-f531204f9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((other_embeddings,), axis=1) # hl_embeddings, months_embeddings, week_embeddings, \n",
    "y = np.array(df['ctr'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c0ccc2d1-2eda-4efc-bcff-589d536b3b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 36), (60000,))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5168f0f3-1dfc-47ef-aa71-486366d154e3",
   "metadata": {},
   "source": [
    "# Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "caeec389-1624-490c-b9bc-73accc0d264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PackagesDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Prepare the Packages dataset for regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "      Multilayer Perceptron for regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "          Forward pass\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f6b66885-48bf-4f28-a764-ac000915368e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.936\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.824\n",
      "Starting epoch 2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.842\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.774\n",
      "Starting epoch 3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.797\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.746\n",
      "Starting epoch 4\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.765\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.728\n",
      "Starting epoch 5\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.738\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.712\n",
      "Starting epoch 6\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.711\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.696\n",
      "Starting epoch 7\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.686\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.683\n",
      "Starting epoch 8\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.661\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.668\n",
      "Starting epoch 9\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.635\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.656\n",
      "Starting epoch 10\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.611\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.644\n",
      "Starting epoch 11\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.587\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.634\n",
      "Starting epoch 12\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.563\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.626\n",
      "Starting epoch 13\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.540\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.616\n",
      "Starting epoch 14\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.519\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.609\n",
      "Starting epoch 15\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.500\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.604\n",
      "Starting epoch 16\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.482\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.597\n",
      "Starting epoch 17\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.465\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.591\n",
      "Starting epoch 18\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.450\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.588\n",
      "Starting epoch 19\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.435\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.583\n",
      "Starting epoch 20\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.422\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.581\n",
      "Starting epoch 21\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.410\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.575\n",
      "Starting epoch 22\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.398\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.571\n",
      "Starting epoch 23\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.387\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.572\n",
      "Starting epoch 24\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.377\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.570\n",
      "Starting epoch 25\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.367\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.566\n",
      "Starting epoch 26\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.358\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.567\n",
      "Starting epoch 27\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.350\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.564\n",
      "Starting epoch 28\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.342\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.564\n",
      "Starting epoch 29\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.334\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.567\n",
      "Starting epoch 30\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Train Loss: 0.327\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Test Loss: 0.562\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCHS = 30\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load Packages dataset\n",
    "# X, y = load_packages()\n",
    "\n",
    "# Prepare Packages dataset\n",
    "dataset = PackagesDataset(X, y)\n",
    "\n",
    "# Split Packages dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create trainloader object\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=1)\n",
    "\n",
    "# Create testloader object\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=1)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP(in_size=X.shape[1])\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "losses = {'train': [], 'test': []}\n",
    "epochs = range(NB_EPOCHS)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "     # Print epoch\n",
    "    print(f'Starting epoch {epoch + 1}')\n",
    "\n",
    "    mlp.train()\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    \n",
    "    current_loss /= len(trainloader)\n",
    "    \n",
    "    losses['train'].append(current_loss)\n",
    "    \n",
    "    print('Train Loss: %.3f'%(current_loss))\n",
    "\n",
    "    \n",
    "def test(epoch):\n",
    "    mlp.eval()\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    \n",
    "    current_loss /= len(testloader)\n",
    "    \n",
    "    losses['test'].append(current_loss)\n",
    "    \n",
    "    print('Test Loss: %.3f'%(current_loss))\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in epochs:\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3554cc1f-00f6-407f-bb62-40afe6d6a97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses['train'])\n",
    "plt.plot(losses['test'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend(['Train','Valid'])\n",
    "plt.title('Train vs Valid Losses')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60bb80d1-56a0-4f97-9086-2b4cfdd9806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5614742592294165"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(losses['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "751625ac-32fc-459f-8f20-bf6a502ebbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.013394257926217"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aug(x, y):\n",
    "    return 100*abs(x - y)/y\n",
    "\n",
    "aug(5692112896036594, 6325511279258322)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c24b076-50b9-4582-920f-4c13b1517040",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "452defb8-952b-4d96-af82-22f22b2c98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "    'Oxford is better than Harvad, Here is why.',\n",
    "    'Oxford is better than Harvad, Here is why.',\n",
    "    'Prince of Sussex is having dinner in Paris tonight.',\n",
    "    'Prince of Sussex is having dinner in Paris tonight.',\n",
    "    'Happy new year!',\n",
    "    'Happy new year!',\n",
    "]\n",
    "\n",
    "dates = np.array([\n",
    "    [1, 1],\n",
    "    [4, 4]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "146908dc-b926-419f-bc52-3432bc7b87d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80f15857ce34c679d0a06681ab7fc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hl_embeddings = embeddings.bert(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e0b8164-3e82-4760-8c41-c4263f1c4c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7x384 and 385x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14772/1273351727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhl_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/citadel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14772/2535914594.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mForward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/citadel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citadel/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citadel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citadel/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citadel/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7x384 and 385x64)"
     ]
    }
   ],
   "source": [
    "X = np.array(hl_embeddings)\n",
    "\n",
    "mlp(torch.tensor(X))*sigma + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50167dbd-e5e4-44ce-9924-0e22ed1f45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mlp.state_dict(), '../output/mlp_global_prediction_500_epochs.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citadel",
   "language": "python",
   "name": "citadel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
